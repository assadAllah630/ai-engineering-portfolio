# Import necessary libraries for the YouTube botnimport gradio as grnimport re  #For extracting video id nfrom youtube_transcript_api import YouTubeTranscriptApi  # For extracting transcripts from YouTube videosnfrom langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable segmentsnfrom ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes  # For specifying model typesnfrom ibm_watsonx_ai import APIClient, Credentials  # For API client and credentials managementnfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams  # For managing model parametersnfrom ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods  # For defining decoding methodsnfrom langchain_ibm import WatsonxLLM, WatsonxEmbeddings  # For interacting with IBM's LLM and embeddingsnfrom ibm_watsonx_ai.foundation_models.utils import get_embedding_model_specs  # For retrieving model specificationsnfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes  # For specifying types of embeddingsnfrom langchain_community.vectorstores import FAISS  # For efficient vector storage and similarity searchnfrom langchain.chains import LLMChain  # For creating chains of operations with LLMsnfrom langchain.prompts import PromptTemplate  # For defining prompt templatesnndef get_video_id(url):    n    # Regex pattern to match YouTube video URLsn    pattern = r'https:\/\/www\.youtube\.com\/watch\?v=([a-zA-Z0-9_-]{11})'n    match = re.search(pattern, url)n    return match.group(1) if match else Nonenndef get_transcript(url):n    # Extracts the video ID from the URLn    video_id = get_video_id(url)nn    # Create a YouTubeTranscriptApi() objectn    ytt_api = YouTubeTranscriptApi()n    n    # Fetch the list of available transcripts for the given YouTube videon    transcripts = ytt_api.list(video_id)n    n    transcript = ""n    for t in transcripts:n        # Check if the transcript's language is Englishn        if t.language_code == 'en':n            if t.is_generated:n                # If no transcript has been set yet, use the auto-generated onen                if len(transcript) == 0:n                    transcript = t.fetch()n            else:n                # If a manually created transcript is found, use it (overrides auto-generated)n                transcript = t.fetch()n                break  # Prioritize the manually created transcript, exit the loopn    n    return transcript if transcript else Nonennndef process(transcript):n    # Initialize an empty string to hold the formatted transcriptn    txt = ""n    n    # Loop through each entry in the transcriptn    for i in transcript:n        try:n            # Append the text and its start time to the output stringn            #txt += f"Text: {i['text']} Start: {i['start']}\n"n            txt += f"Text: {i.text} Start: {i.start}\n"n        except KeyError:n            # If there is an issue accessing 'text' or 'start', skip this entryn            passn            n    # Return the processed transcript as a single stringn    return txtnndef chunk_transcript(processed_transcript, chunk_size=200, chunk_overlap=20):n    # Initialize the RecursiveCharacterTextSplitter with specified chunk size and overlapn    text_splitter = RecursiveCharacterTextSplitter(n        chunk_size=chunk_size,n        chunk_overlap=chunk_overlapn    )nn    # Split the transcript into chunksn    chunks = text_splitter.split_text(processed_transcript)n    return chunksnnndef setup_credentials():n    # Define the model ID for the WatsonX model being usedn    model_id = "meta-llama/llama-3-2-3b-instruct"n    n    # Set up the credentials by specifying the URL for IBM Watson servicesn    credentials = Credentials(url="https://us-south.ml.cloud.ibm.com")n    n    # Create an API client using the credentialsn    client = APIClient(credentials)n    n    # Define the project ID associated with the WatsonX platformn    project_id = "skills-network"n    n    # Return the model ID, credentials, client, and project ID for later usen    return model_id, credentials, client, project_idnndef define_parameters():n    # Return a dictionary containing the parameters for the WatsonX modeln    return {n        # Set the decoding method to GREEDY for generating textn        GenParams.DECODING_METHOD: DecodingMethods.GREEDY,n        n        # Specify the maximum number of new tokens to generaten        GenParams.MAX_NEW_TOKENS: 900,n    }nnndef initialize_watsonx_llm(model_id, credentials, project_id, parameters):n    # Create and return an instance of the WatsonxLLM with the specified configurationn    return WatsonxLLM(n        model_id=model_id,          # Set the model ID for the LLMn        url=credentials.get("url"),      # Retrieve the service URL from credentialsn        project_id=project_id,            # Set the project ID for accessing resourcesn        params=parameters                  # Pass the parameters for model behaviorn    )nnndef setup_embedding_model(credentials, project_id):n    # Create and return an instance of WatsonxEmbeddings with the specified configurationn    return WatsonxEmbeddings(n        model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,  # Set the model ID for the SLATE-30M embedding modeln        url=credentials["url"],                            # Retrieve the service URL from the provided credentialsn        project_id=project_id                               # Set the project ID for accessing resources in the Watson environmentn    )nnndef create_faiss_index(chunks, embedding_model):n    """n    Create a FAISS index from text chunks using the specified embedding model.n    n    :param chunks: List of text chunksn    :param embedding_model: The embedding model to usen    :return: FAISS indexn    """n    # Use the FAISS library to create an index from the provided text chunksn    return FAISS.from_texts(chunks, embedding_model)nnndef perform_similarity_search(faiss_index, query, k=3):n    """n    Search for specific queries within the embedded transcript using the FAISS index.n    n    :param faiss_index: The FAISS index containing embedded text chunksn    :param query: The text input for the similarity searchn    :param k: The number of similar results to return (default is 3)n    :return: List of similar resultsn    """n    # Perform the similarity search using the FAISS indexn    results = faiss_index.similarity_search(query, k=k)n    return resultsnnndef create_summary_prompt():n    """n    Create a PromptTemplate for summarizing a YouTube video transcript.n    n    :return: PromptTemplate objectn    """n    # Define the template for the summary promptn    template = """n    <|begin_of_text|><|start_header_id|>system<|end_header_id|>n    You are an AI assistant tasked with summarizing YouTube video transcripts. Provide concise, informative summaries that capture the main points of the video content.nn    Instructions:n    1. Summarize the transcript in a single concise paragraph.n    2. Ignore any timestamps in your summary.n    3. Focus on the spoken content (Text) of the video.nn    Note: In the transcript, "Text" refers to the spoken words in the video, and "start" indicates the timestamp when that part begins in the video.<|eot_id|><|start_header_id|>user<|end_header_id|>n    Please summarize the following YouTube video transcript:nn    {transcript}<|eot_id|><|start_header_id|>assistant<|end_header_id|>n    """n    n    # Create the PromptTemplate object with the defined templaten    prompt = PromptTemplate(n        input_variables=["transcript"],n        template=templaten    )n    n    return promptnnndef create_summary_chain(llm, prompt, verbose=True):n    """n    Create an LLMChain for generating summaries.n    n    :param llm: Language model instancen    :param prompt: PromptTemplate instancen    :param verbose: Boolean to enable verbose output (default: True)n    :return: LLMChain instancen    """n    return LLMChain(llm=llm, prompt=prompt, verbose=verbose)nnndef retrieve(query, faiss_index, k=7):n    """n    Retrieve relevant context from the FAISS index based on the user's query.nn    Parameters:n        query (str): The user's query string.n        faiss_index (FAISS): The FAISS index containing the embedded documents.n        k (int, optional): The number of most relevant documents to retrieve (default is 3).nn    Returns:n        list: A list of the k most relevant documents (or document chunks).n    """n    relevant_context = faiss_index.similarity_search(query, k=k)n    return relevant_contextnndef create_qa_prompt_template():n    """n    Create a PromptTemplate for question answering based on video content.n    Returns:n        PromptTemplate: A PromptTemplate object configured for Q&A tasks.n    """n    n    # Define the template stringn    qa_template = """n    <|begin_of_text|><|start_header_id|>system<|end_header_id|>n    You are an expert assistant providing detailed and accurate answers based on the following video content. Your responses should be:n    1. Precise and free from repetitionn    2. Consistent with the information provided in the videon    3. Well-organized and easy to understandn    4. Focused on addressing the user's question directlyn    If you encounter conflicting information in the video content, use your best judgment to provide the most likely correct answer based on context.n    Note: In the transcript, "Text" refers to the spoken words in the video, and "start" indicates the timestamp when that part begins in the video.<|eot_id|>nn    <|start_header_id|>user<|end_header_id|>n    Relevant Video Context: {context}n    Based on the above context, please answer the following question:n    {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>n    """n    # Create the PromptTemplate objectn    prompt_template = PromptTemplate(n        input_variables=["context", "question"],n        template=qa_templaten    )n    return prompt_templatennndef create_qa_chain(llm, prompt_template, verbose=True):n    """n    Create an LLMChain for question answering.nn    Args:n        llm: Language model instancen            The language model to use in the chain (e.g., WatsonxGranite).n        prompt_template: PromptTemplaten            The prompt template to use for structuring inputs to the language model.n        verbose: bool, optional (default=True)n            Whether to enable verbose output for the chain.nn    Returns:n        LLMChain: An instantiated LLMChain ready for question answering.n    """n    n    return LLMChain(llm=llm, prompt=prompt_template, verbose=verbose)nnndef generate_answer(question, faiss_index, qa_chain, k=7):n    """n    Retrieve relevant context and generate an answer based on user input.nn    Args:n        question: strn            The user's question.n        faiss_index: FAISSn            The FAISS index containing the embedded documents.n        qa_chain: LLMChainn            The question-answering chain (LLMChain) to use for generating answers.n        k: int, optional (default=3)n            The number of relevant documents to retrieve.nn    Returns:n        str: The generated answer to the user's question.n    """nn    # Retrieve relevant contextn    relevant_context = retrieve(question, faiss_index, k=k)nn    # Generate answer using the QA chainn    answer = qa_chain.predict(context=relevant_context, question=question)nn    return answernnn# Initialize an empty string to store the processed transcript after fetching and preprocessingnprocessed_transcript = ""nndef summarize_video(video_url):n    """n    Title: Summarize Videonn    Description:n    This function generates a summary of the video using the preprocessed transcript.n    If the transcript hasn't been fetched yet, it fetches it first.nn    Args:n        video_url (str): The URL of the YouTube video from which the transcript is to be fetched.nn    Returns:n        str: The generated summary of the video or a message indicating that no transcript is available.n    """n    global fetched_transcript, processed_transcriptn    n    n    if video_url:n        # Fetch and preprocess transcriptn        fetched_transcript = get_transcript(video_url)n        processed_transcript = process(fetched_transcript)n    else:n        return "Please provide a valid YouTube URL."nn    if processed_transcript:n        # Step 1: Set up IBM Watson credentialsn        model_id, credentials, client, project_id = setup_credentials()nn        # Step 2: Initialize WatsonX LLM for summarizationn        llm = initialize_watsonx_llm(model_id, credentials, project_id, define_parameters())nn        # Step 3: Create the summary prompt and chainn        summary_prompt = create_summary_prompt()n        summary_chain = create_summary_chain(llm, summary_prompt)nn        # Step 4: Generate the video summaryn        summary = summary_chain.run({"transcript": processed_transcript})n        return summaryn    else:n        return "No transcript available. Please fetch the transcript first."nnndef answer_question(video_url, user_question):n    """n    Title: Answer User's Questionnn    Description:n    This function retrieves relevant context from the FAISS index based on the user's query n    and generates an answer using the preprocessed transcript.n    If the transcript hasn't been fetched yet, it fetches it first.nn    Args:n        video_url (str): The URL of the YouTube video from which the transcript is to be fetched.n        user_question (str): The question posed by the user regarding the video.nn    Returns:n        str: The answer to the user's question or a message indicating that the transcript n             has not been fetched.n    """n    global fetched_transcript, processed_transcriptnn    # Check if the transcript needs to be fetchedn    if not processed_transcript:n        if video_url:n            # Fetch and preprocess transcriptn            fetched_transcript = get_transcript(video_url)n            processed_transcript = process(fetched_transcript)n        else:n            return "Please provide a valid YouTube URL."nn    if processed_transcript and user_question:n        # Step 1: Chunk the transcript (only for Q&A)n        chunks = chunk_transcript(processed_transcript)nn        # Step 2: Set up IBM Watson credentialsn        model_id, credentials, client, project_id = setup_credentials()nn        # Step 3: Initialize WatsonX LLM for Q&An        llm = initialize_watsonx_llm(model_id, credentials, project_id, define_parameters())nn        # Step 4: Create FAISS index for transcript chunks (only needed for Q&A)n        embedding_model = setup_embedding_model(credentials, project_id)n        faiss_index = create_faiss_index(chunks, embedding_model)nn        # Step 5: Set up the Q&A prompt and chainn        qa_prompt = create_qa_prompt_template()n        qa_chain = create_qa_chain(llm, qa_prompt)nn        # Step 6: Generate the answer using FAISS indexn        answer = generate_answer(user_question, faiss_index, qa_chain)n        return answern    else:n        return "Please provide a valid question and ensure the transcript has been fetched."nnnwith gr.Blocks() as interface:nn    gr.Markdown(n        "<h2 style='text-align: center;'>YouTube Video Summarizer and Q&A</h2>"n    )nn    # Input field for YouTube URLn    video_url = gr.Textbox(label="YouTube Video URL", placeholder="Enter the YouTube Video URL")n    n    # Outputs for summary and answern    summary_output = gr.Textbox(label="Video Summary", lines=5)n    question_input = gr.Textbox(label="Ask a Question About the Video", placeholder="Ask your question")n    answer_output = gr.Textbox(label="Answer to Your Question", lines=5)nn    # Buttons for selecting functionalities after fetching transcriptn    summarize_btn = gr.Button("Summarize Video")n    question_btn = gr.Button("Ask a Question")nn    # Display status message for transcript fetchn    transcript_status = gr.Textbox(label="Transcript Status", interactive=False)nn    # Set up button actionsn    summarize_btn.click(summarize_video, inputs=video_url, outputs=summary_output)n    question_btn.click(answer_question, inputs=[video_url, question_input], outputs=answer_output)nn# Launch the app with specified server name and portninterface.launch(server_name="0.0.0.0", server_port=7860)